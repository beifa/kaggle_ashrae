{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Script_v2_by_meter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beifa/kaggle_ashrae/blob/master/Script_v2_by_meter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGxC1CH4SCVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "from lightgbm import LGBMRegressor\n",
        "import lightgbm as lgb\n",
        "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.metrics import mean_squared_log_error, mean_squared_error\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import catboost\n",
        "from catboost import Pool, cv\n",
        "from catboost import CatBoostRegressor\n",
        "print(catboost.__version__)\n",
        "#\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "def input_file(file):\n",
        "    path = f\"../input/ashrae-energy-prediction/{file}\"\n",
        "    if not os.path.exists(path): return path + \".gz\"\n",
        "    return path\n",
        "\n",
        "#compress\n",
        "def compress_dataframe(df):\n",
        "    \"\"\"\n",
        "    не знаю надо будет попробовать, что такой подход проще чем указывать\n",
        "    и проверять данные для уменьшения размера(по памяти).\n",
        "    А тут пандас само это делает downcast: {\"целое число\", \"подписано\", \"без знака\", \"плавать\"}, начинает с мин и тд.\n",
        "    train - mem used - 616 after compres -173\n",
        "    \"\"\"\n",
        "    result = df.copy()\n",
        "    for col in result.columns:\n",
        "        col_data = result[col]\n",
        "        dn = col_data.dtype.name    \n",
        "        if dn == \"object\":\n",
        "          \"\"\"\n",
        "          make category and return cat int      \n",
        "          \"\"\"\n",
        "          result[col] = pd.to_numeric(col_data.astype(\"category\").cat.codes, downcast=\"integer\")\n",
        "        elif dn == \"bool\":\n",
        "          result[col] = col_data.astype(\"int8\")\n",
        "        elif dn.startswith(\"int\") or (col_data.round() == col_data).all():\n",
        "          result[col] = pd.to_numeric(col_data, downcast=\"integer\")\n",
        "        else:\n",
        "          result[col] = pd.to_numeric(col_data, downcast='float')\n",
        "    return result\n",
        "\n",
        "#weather\n",
        "\n",
        "def fill_by_each_id(df):\n",
        "    \"\"\"\n",
        "    идею подглядел, можно время преобразовать в часы и сделать после смещение для выравнивания по зонам\n",
        "    но можно сделать и иначе просто сделать из времени категории(на выходе одно и тоже)\n",
        "    \"\"\"\n",
        "    offsets = [-5, 0, -7, -5, -8, 0, -5, -5, -5, -6, -7, -5, 0, -6, -5, -5]#на сколько время отклоняется для каждого сайта  \n",
        "    df.timestamp = df.timestamp.astype(\"datetime64[ns]\")\n",
        "    df.timestamp = (df.timestamp- pd.to_datetime('2016-01-01')).dt.total_seconds() // 3600\n",
        "    dict_offset = {site:offset for site, offset in enumerate(offsets)} #{0: -5, 1: 0,...}\n",
        "    df.timestamp = df.timestamp - df.site_id.map(dict_offset)\n",
        "    box = []\n",
        "    for iid in df.site_id.unique(): \n",
        "        site = df[df.site_id == iid].set_index(['timestamp']).reindex(range(8784))\n",
        "        site.site_id = iid #fill site id\n",
        "        for col in [c for c in site.columns if c  != 'site_id']: # all col without site_id\n",
        "            site[f'mark_not_fill_{col}'] = ~site[col].isna() # return not na\n",
        "            site[col] = site[col].interpolate(limit_direction='both', method='linear')\n",
        "            site[col] = site[col].fillna(df[col].median())\n",
        "        box.append(site)# after we concat site_id : 0, 1, 15 \n",
        "    df = pd.concat(box).reset_index()\n",
        "    return compress_dataframe(df).set_index([\"site_id\", \"timestamp\"])\n",
        "\n",
        "#for test\n",
        "\n",
        "def fill_by_each_id_test(df):\n",
        "    \"\"\"\n",
        "    df = weather\n",
        "    df == 1 year train but test split for bool by two years\n",
        "    year = '2016-01-01'(train), count = 8784\n",
        "    year = '2017-01-01'(test), count = 8599\n",
        "    year = '2018-01-01'(test), count = 3079 not all year\n",
        "    ======================ps\n",
        "    карочь не хватает памяти дропается, + много дубликатов но и вот дропнуть их не могу по памяти не получается такой подход и вот\n",
        "    темным вечерком пришла мысля такая timestamp - 2016 на тесте начинается с 8789 и кончается 26308 и вот оно решение\n",
        "    ======================\n",
        "    \"\"\"\n",
        "    offsets = [-5, 0, -7, -5, -8, 0, -5, -5, -5, -6, -7, -5, 0, -6, -5, -5]#на сколько время отклоняется для каждого сайта  \n",
        "    df.timestamp = df.timestamp.astype(\"datetime64[ns]\")\n",
        "    df.timestamp = (df.timestamp- pd.to_datetime('2016-01-01')).dt.total_seconds() // 3600\n",
        "    dict_offset = {site:offset for site, offset in enumerate(offsets)} #{0: -5, 1: 0,...}\n",
        "    df.timestamp = df.timestamp - df.site_id.map(dict_offset)\n",
        "    #return df \n",
        "    box = []\n",
        "    for iid in df.site_id.unique():\n",
        "        site = df[df.site_id == iid].set_index(['timestamp']).reindex(range(8789, 26308))\n",
        "        site.site_id = iid #fill site id\n",
        "        for col in [c for c in site.columns if c  != 'site_id']: # all col without site_id\n",
        "            site[f'mark_not_fill_{col}'] = ~site[col].isna() # return not na\n",
        "            site[col] = site[col].interpolate(limit_direction='both', method='linear')\n",
        "            site[col] = site[col].fillna(df[col].median())\n",
        "        box.append(site)# after we concat site_id : 0, 1, 15 \n",
        "    df = pd.concat(box).reset_index()\n",
        "    return compress_dataframe(df).set_index([\"site_id\", \"timestamp\"])\n",
        "\n",
        "#drop_0\n",
        "\n",
        "def drop_0(df):\n",
        "    \"\"\" \n",
        "    df = train data\n",
        "\n",
        "    идея тут проста электричество не может быть 0, для коммерческих построек\n",
        "    там полюбому даже если никто не работает или ночь или хз что тратится энергия,\n",
        "    от общего числа данных эти значения составляют 0,02%.\n",
        "    Альтернатива добавить сюда другие счетчики дабы исключить все\"\"\"\n",
        "    df = df.reindex(df[(df.meter_reading > 0) & (df.meter >= 0)].index) #0.9073027933181969 \n",
        "    return  df\n",
        "\n",
        "#fill metadata\n",
        "\n",
        "def fill_metadata(name):\n",
        "    \"\"\"\n",
        "    name = name file \"building_metadata.csv\"\n",
        "\n",
        "    Food sales and service (2005 - 2012 )\n",
        "    сюда входят рестораны, ночные клубы, фастфуд, супермаркеты и тд.\n",
        "    Я думаю что в большинстве своем это одноэтажные здания и мы установим значение на 2.\n",
        "    Religious worship (< 1980)\n",
        "    церкви, храмы, мечети, синагоги, дома собраний или любые другие здания,\n",
        "    которые в основном служат местом религиозного поклонения значение 1\n",
        "    Services (2003-2007-2012)\n",
        "    Data Center, Personal Services (Health/Beauty, Dry Cleaning, etc), Repair Services (Vehicle, Shoe, Locksmith, etc) значение 1.5\n",
        "    Радует не большое количество зданий 5, 3, 10    \n",
        "    \"\"\"\n",
        "    metadata = pd.read_csv(input_file(name))\n",
        "    m = metadata.primary_use == 'Food sales and service'\n",
        "    metadata.loc[m, 'year_built'] = [2012, np.nan, 2005, np.nan, 2009]\n",
        "    metadata.loc[m, 'floor_count'] = [2, np.nan, 1, np.nan, 2]\n",
        "    #\n",
        "    m2 =  metadata.primary_use == 'Religious worship'\n",
        "    metadata.loc[m2, 'year_built'] = [1970, 1980, 1930]\n",
        "    metadata.loc[m2, 'floor_count'] = [1, 2, 2]\n",
        "    #\n",
        "    m3 =  metadata.primary_use == 'Services'\n",
        "    metadata.loc[m3, 'year_built'] = [2003, 2007, np.nan, 2005, np.nan, 2012, np.nan, np.nan, 2009, np.nan]\n",
        "    metadata.loc[m3, 'floor_count'] = [3, 1, np.nan, 2, np.nan, 1, np.nan, 3, 2, np.nan]\n",
        "    ##\n",
        "    metadata['year_built'] = metadata['year_built'].interpolate()\n",
        "    metadata['floor_count'] = metadata['floor_count'].interpolate(method = 'pad')\n",
        "    #в начале много пропусков  мином заполним\n",
        "    m = metadata['floor_count'].isnull()\n",
        "    metadata.loc[m, 'floor_count'] = int(metadata['floor_count'].mean())\n",
        "    metadata['year_built'] = metadata['year_built'].astype('int')\n",
        "    return metadata\n",
        "\n",
        "#####MERGE\n",
        "\n",
        "def read_building_metadata():\n",
        "    return compress_dataframe(fill_metadata('building_metadata.csv')).set_index(\"building_id\")\n",
        "    \n",
        "def read_test():\n",
        "    df = pd.read_csv(input_file(\"test.csv\"), parse_dates=[\"timestamp\"])\n",
        "    df.timestamp = (df.timestamp - pd.to_datetime(\"2016-01-01\")).dt.total_seconds() // 3600\n",
        "    return compress_dataframe(df).set_index(\"row_id\")\n",
        "\n",
        "def mergeall(name, weather, train = True, save = False): \n",
        "    ##train --> metadata-->weather \n",
        "    weather = pd.read_csv(input_file(weather)) \n",
        "    if train:\n",
        "        weather = fill_by_each_id(weather)\n",
        "        data = drop_0(compress_dataframe(pd.read_csv(input_file(name))))\n",
        "    else:\n",
        "        weather = fill_by_each_id_test(weather)\n",
        "        data = read_test()\n",
        "    df = data.join(read_building_metadata(), on=\"building_id\") \\\n",
        "             .join(weather, on=[\"site_id\", \"timestamp\"]).fillna(-1)#.to_pickle(path + 'data_test_before_fake.pkl') # -9999\n",
        "    \n",
        "    #no time to explain just do it\n",
        "    bad_col = [\n",
        "    'mark_not_fill_cloud_coverage',\n",
        "    'mark_not_fill_precip_depth_1_hr',\n",
        "    'mark_not_fill_sea_level_pressure',\n",
        "    'mark_not_fill_wind_direction']    \n",
        "    \n",
        "    df.drop(bad_col, axis = 'columns', inplace = True)    \n",
        "    \n",
        "    del data\n",
        "    del weather\n",
        "    gc.collect()\n",
        "\n",
        "    if save:\n",
        "        #Save but crush memory\n",
        "        df.to_pickle(path + 'data_test_before_fake.pkl')\n",
        "        return print('Saved not return df pls load instance')\n",
        "    return df\n",
        "\n",
        "#merged data\n",
        "data_train = mergeall('train.csv', 'weather_train.csv')\n",
        "data_test = compress_dataframe(mergeall('test.csv', 'weather_test.csv', train=False, save = False))\n",
        "\n",
        "print('Data Merged')\n",
        "\n",
        "\n",
        "def drop_fake_site(df):\n",
        "    #train\n",
        "    #141days, after merge data\n",
        "    df = df[(df.timestamp >= 3378) | (df.site_id != 0) | (df.meter != 0)]\n",
        "    return df\n",
        "\n",
        "def make_time(idx, namefile, savename):\n",
        "    \"\"\"\n",
        "    name = file name to load\n",
        "    le train after drop 0 meter  \n",
        "    maybe saved ?\n",
        "    \"\"\"\n",
        "    temp = pd.read_csv(input_file(namefile))\n",
        "    time = temp.loc[idx.index].timestamp\n",
        "    del temp\n",
        "    gc.collect()\n",
        "    time = pd.to_datetime(time)\n",
        "    df = pd.DataFrame(time, columns=['timestamp'])\n",
        "    col = 'timestamp'   \n",
        "    df['weekday'] = df[col].dt.weekday.astype(np.uint8)\n",
        "    df['dayofyear'] = df[col].dt.dayofyear.astype(np.uint16) - 1    \n",
        "    del time\n",
        "    gc.collect()\n",
        "    df.drop('timestamp', axis = 'columns', inplace=True) #нам не нужно время по умолчанию дропаем, только  признаки\n",
        "    #df.to_pickle(path + savename)    \n",
        "    \"\"\"\n",
        "    у нас теперь есть время отдельно \n",
        "    \"\"\"    \n",
        "    return df\n",
        "\n",
        "print('Make time features')\n",
        "\n",
        "def humidity(df):\n",
        "    #df = data with weather  \n",
        "    \n",
        "    df['e'] = df['air_temperature'].apply(lambda x: 6.11 * 10**( (7.5 * x) / (237.7 + x) ))\n",
        "    df['es'] = df['dew_temperature'].apply(lambda x: 6.11 * 10**( (7.5 * x) / (237.7 + x) ))\n",
        "    df['rh'] =((df['es'] /  df['e'])) * 100\n",
        "    #add recommends\n",
        "    df['rec_air'] = pd.cut(df.air_temperature, [-50, 0, 20, 25, 60],\n",
        "                           labels=['very_bad_cold', 'bad', 'Good', 'very_bad_hot'],\n",
        "                           include_lowest=True)  \n",
        "#     df['rec_dew'] = pd.cut(df.dew_temperature, [-50, -20, -5,  16, 20, 50],\n",
        "#                            labels=['dew_very_low','dew_low', 'Good', 'dew_big', 'dew_very_big'],\n",
        "#                            include_lowest=True)\n",
        "    #need check rh not big 100\n",
        "    #df.loc[df.rh >= 100, 'rh'] = 100\n",
        "    #df['rec_rh'] = pd.cut(df.rh, [0,  20, 60, 100],labels=['small_h', 'Good', 'big_h'],  include_lowest=True)    \n",
        "    return df\n",
        "\n",
        "print('Make humidity')\n",
        "\n",
        "def add_new_f(data_train, data_test):\n",
        "    train_new = drop_fake_site(data_train)\n",
        "    del data_train\n",
        "    #time\n",
        "    temp_train = make_time(train_new, 'train.csv', savename = 'time_train.pkl.gz')\n",
        "    temp_test  = make_time(data_test, 'test.csv', savename = 'time_test.pkl.gz')\n",
        "    \n",
        "    train = humidity(pd.concat([train_new, temp_train], axis = 'columns'))\n",
        "    test = humidity(pd.concat([data_test, temp_test], axis = 'columns'))\n",
        "    del temp_train\n",
        "    del temp_test\n",
        "    del data_test\n",
        "    gc.collect()      \n",
        "    return train, test\n",
        "\n",
        "train, test = add_new_f(data_train, data_test)\n",
        "\n",
        "print('Merge new features')\n",
        "\n",
        "#i need more gold :)\n",
        "\n",
        "#add cat\n",
        "for_cat = ['building_id', 'meter', 'site_id', 'primary_use', 'floor_count', 'cloud_coverage',\n",
        "              'mark_not_fill_air_temperature', 'mark_not_fill_dew_temperature', 'mark_not_fill_wind_speed']\n",
        "\n",
        "\n",
        "final_col = ['building_id', 'meter', 'timestamp', 'meter_reading', 'site_id',\n",
        "             'primary_use', 'square_feet', 'year_built', 'floor_count',\n",
        "             'cloud_coverage', 'dew_temperature', 'sea_level_pressure',\n",
        "             'wind_direction', 'wind_speed', 'mark_not_fill_air_temperature',\n",
        "             'mark_not_fill_dew_temperature', 'mark_not_fill_wind_speed', 'weekday',\n",
        "             'dayofyear', 'rh', 'rec_air']\n",
        "\n",
        "final_col_test = ['building_id', 'meter', 'timestamp', 'site_id',\n",
        "                  'primary_use', 'square_feet', 'year_built', 'floor_count',\n",
        "                  'cloud_coverage', 'dew_temperature', 'sea_level_pressure',\n",
        "                  'wind_direction', 'wind_speed', 'mark_not_fill_air_temperature',\n",
        "                  'mark_not_fill_dew_temperature', 'mark_not_fill_wind_speed', 'weekday',\n",
        "                  'dayofyear', 'rh', 'rec_air']\n",
        "\n",
        "#bad site\n",
        "def find_bad_building1099(df):\n",
        "    #data = df\n",
        "    #3351 row\n",
        "    return df[(df.building_id == 1099) & (df.meter == 2) & (df.meter_reading > 3e4)].index\n",
        "\n",
        "def drop_make_cat(data, col, cat_col, train= True):\n",
        "    #data - data\n",
        "    #col = col for droped\n",
        "    d = data.copy()\n",
        "    \n",
        "    #cat error fetures in kaggle not error in colab    ['cloud_coverage', 'floor_count']\n",
        "    d.loc[:,'floor_count'] = d.loc[:,'floor_count'].astype('int')\n",
        "    d.loc[:,'cloud_coverage'] = d.loc[:,'cloud_coverage'].astype('int')\n",
        "    \n",
        "    for c in cat_col:        \n",
        "        d[c] = d[c].astype('category')         \n",
        "    new_d = d.loc[:, col]\n",
        "    del data\n",
        "    del d\n",
        "    gc.collect()\n",
        "    \n",
        "    if train:\n",
        "        print(new_d.shape)\n",
        "        idx_bad = find_bad_building1099(new_d)\n",
        "        new_d = new_d.drop(idx_bad)\n",
        "        print(new_d.shape)        \n",
        "    \n",
        "    new_d.rec_air = new_d.rec_air.cat.codes.astype(\"category\")    \n",
        "    return new_d      \n",
        "    \n",
        "\n",
        "train_new = drop_make_cat(train, final_col, for_cat, train = True)\n",
        "test_new = drop_make_cat(test, final_col_test, for_cat, train= False)\n",
        "\n",
        "\n",
        "del test\n",
        "del train\n",
        "gc.collect()\n",
        "\n",
        "print('Droped not cool features')\n",
        "\n",
        "#for test for small sample data\n",
        "# def rand_sample(train, f = 0.25):\n",
        "#     #вернет выборку рандомную, f - коеф. размера выборки\n",
        "#     np.random.seed(0)\n",
        "#     idx = np.argsort(train.timestamp.values, kind='stable')\n",
        "#     sample_idx = np.random.choice(idx, int(len(idx) * f), replace=False) \n",
        "#     return sample_idx\n",
        "\n",
        "# sample_train = train_new.iloc[rand_sample(train_new, f = 0.01)] #4584686 rows × 45 columns\n",
        "\n",
        "# X = sample_train.drop('meter_reading', axis = 1)\n",
        "# y = sample_train.meter_reading\n",
        "\n",
        "# del train_new\n",
        "# del test_new\n",
        "# gc.collect()\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=13)\n",
        "# train_new = pd.concat([X_train, y_train], axis = 1)\n",
        "# test_new = X_test\n",
        "# print('Sample data maked ends')\n",
        "\n",
        "\n",
        "\n",
        "################### ADD Class \n",
        "\n",
        "\"\"\"\n",
        "I didn’t have enough time to test small changes, but preliminary assessments are worse, they are relearned very much.\n",
        "This was to be expected .......\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class by_Meter(BaseEstimator, RegressorMixin):\n",
        "  def __init__(self, model, col):\n",
        "    self.model = model\n",
        "    self.col = col\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    self.fitted = {}\n",
        "    importances = []\n",
        "    for val in X[self.col].unique():\n",
        "      #val = 0.1.2.3.4\n",
        "      X1 = X[X[self.col] == val].drop(columns=[self.col])\n",
        "      self.fitted[val] = clone(self.model).fit(X1, y.reindex_like(X1))      \n",
        "      del X1    \n",
        "    return self\n",
        "\n",
        "  def predict(self, X):\n",
        "    result = np.zeros(len(X))\n",
        "    for val in X[self.col].unique():\n",
        "      ix = np.nonzero((X[self.col] == val).to_numpy())     \n",
        "      predictions = self.fitted[val].predict(X.iloc[ix].drop(columns=[self.col]))\n",
        "      result[ix] = predictions\n",
        "    return result  \n",
        "\n",
        "\n",
        "def cluster_id(data, test, mark, model, num_cluster, skip = True):  \n",
        "    \"\"\"\n",
        "    че мы хотим, а мы хотим\n",
        "    разбиваем на  сайт ид и в нем делаем кластеры  затем учим и предсказывае для каждого кластера в сайт ид   \n",
        "    data = data\n",
        "    num_clusters = int, num cluster  \n",
        "    mark = feature test\n",
        "   \n",
        "    \"\"\"\n",
        "    mix_model = by_Meter(model, 'meter') #col split fit\n",
        "\n",
        "    idx_cat = np.where(data.drop(['meter_reading'], axis = 'columns').dtypes == 'category')[0]\n",
        "    idx_cat_test = np.where(test.dtypes == 'category')[0]\n",
        "    idx_cat = np.insert(idx_cat, 0, 0) # add building id\n",
        "    idx_cat_test = np.insert(idx_cat_test, 0, 0) \n",
        "    #print(idx_cat)\n",
        "    data.building_id = data.building_id.astype(np.int16)\n",
        "    test.building_id = test.building_id.astype(np.int16)  \n",
        "    df_id = pd.DataFrame(data = np.zeros(len(test)), columns = ['pred'], index=test.index)\n",
        "    for id_ in data.site_id.unique(): \n",
        "        mask_id = data.site_id == id_\n",
        "        mask_id_test = test.site_id == id_ \n",
        "        site_id = data[mask_id] \n",
        "        site_id_test = test[mask_id_test]   \n",
        "        #we make cluster in site id \n",
        "        if skip:\n",
        "            g = site_id.groupby(mark)['meter_reading'].median() \n",
        "            km = KMeans(n_clusters=num_cluster, init='random', n_init=5, max_iter=100, random_state=13, n_jobs=-1)   \n",
        "            label = km.fit_predict(g.values.reshape(-1, 1)) \n",
        "            g = g.to_frame()    \n",
        "            g['labels'] = label\n",
        "            cluster_df = site_id[mark].map(g['labels']) #i make idx - labels\n",
        "            cluster_test = site_id_test[mark].map(g['labels']) \n",
        "            for num in range(num_cluster):\n",
        "                #num clusters\n",
        "                idx = cluster_df[cluster_df == num].index #index cluster in id_site\n",
        "                #idx_box[id_] = idx #index clusters only site_id\n",
        "                #site_id.loc[cluster_df[cluster_df == 1].index]        \n",
        "                X = site_id.loc[idx].drop('meter_reading', axis = 1)\n",
        "                y = site_id.loc[idx]['meter_reading']      \n",
        "                # learning\n",
        "                # predict for cluster\n",
        "                # join for predict site\n",
        "                # add for predict sample train\n",
        "                idx_test = cluster_test[cluster_test == num].index \n",
        "                test_cluster = site_id_test.loc[idx_test]\n",
        "                mix_model.fit(X, np.log1p(y))\n",
        "                df_id.loc[idx_test, 'pred'] = mix_model.predict(test_cluster)\n",
        "    return df_id\n",
        "\n",
        "\n",
        "#####################Model\n",
        "print('Train and predict')\n",
        "\n",
        "# small iter(need commit),  because a very very long time 1h33min kaggle\n",
        "# for  scored uncomment, iter = 1000 ~ 7h(if = 1500 not ended session in kaggle 9h), i predict fo my Pc iter > 2000 5d\n",
        "\n",
        "num_cluster = 3\n",
        "\n",
        "param_knn = {'algorithm': 'ball_tree',\n",
        "             'n_neighbors': 15, # 50 #100 # 250\n",
        "             'p': 1, \n",
        "             'weights': 'distance'}\n",
        "model_knn = KNeighborsRegressor(**param_knn)\n",
        "# pred_knn = cluster_id('', data_knn, data_knn_test, 'building_id', model_knn, num_cluster, skip = True, param = None)\n",
        "# print('ends')\n",
        "pred_knn = cluster_id(train_new, test_new, 'building_id', model_knn, num_cluster, skip = True)\n",
        "\n",
        "print('Knn predict end')\n",
        "\n",
        "####lgbm\n",
        "\n",
        "params = {'boosting_type': 'gbdt',  \n",
        "          'learning_rate': 0.1,\n",
        "          'max_depth': 3,\n",
        "          'n_estimators': 10, # 1000\n",
        "          'subsample': 0.5,\n",
        "          'subsample_freq': 0.5,\n",
        "          'colsample_bytree':, 0.5,\n",
        "          }\n",
        "\n",
        "model_lgb = lgb.LGBMRegressor(**params, random_state=13) ### CHANGE not SKLERN\n",
        "pred_lgbm = cluster_id(train_new, test_new, 'building_id', model_lgb, num_cluster,  skip = Truem)\n",
        "\n",
        "print('Lgb predict end')\n",
        "####cat\n",
        "\n",
        "param = {'border_count': 237,\n",
        "         'iterations': 10,#1000\n",
        "         #'grow_policy': 'Lossguide',#gpu\n",
        "         'l2_leaf_reg': 16,\n",
        "         'learning_rate': 0.3857377455824425,\n",
        "         'max_depth': 14, \n",
        "         #'max_leaves': 34,#gpu\n",
        "         #'min_data_in_leaf': 75,  #gpu    \n",
        "         'iterations':10,\n",
        "         'random_strength': 6,\n",
        "         'eval_metric': 'RMSE',\n",
        "          'random_seed':13,\n",
        "          'verbose':25,\n",
        "          #'task_type': 'GPU',\n",
        "          'od_type':'Iter',    \n",
        "          'od_wait': 20 }\n",
        "\n",
        "model_cat = CatBoostRegressor(**param)\n",
        "pred_cat = cluster_id(train_new, test_new, 'building_id', model_cat, num_cluster, skip = True)\n",
        "print('Cat predict end')\n",
        "\n",
        "####lasso\n",
        "model_lasso = Lasso(alpha = 1, random_state=13)\n",
        "pred_lasso = cluster_id(train_new, test_new, 'building_id', model_lasso, num_cluster, skip = True)\n",
        "print('Lasso predict end')\n",
        "\n",
        "#long ago in a distant galaxy :))\n",
        "\n",
        "pred = pred_cat * 0.45 + pred_lgbm * 0.25 + pred_knn * 0.15 + pred_lasso *0.15\n",
        "\n",
        "\n",
        "\n",
        "print('Saved predict, still a bit!!')\n",
        "\n",
        "predictions = pd.DataFrame({\n",
        "    \"row_id\": test_new.index,\n",
        "    \"meter_reading\": np.clip(np.expm1(pred['pred']), 0, None)\n",
        "})\n",
        "\n",
        "path = '/kaggle/working/'\n",
        "predictions.to_csv(path + \"submission_f_v2_003.csv\", index=False, float_format=\"%.4f\")\n",
        "print('Script complite!')\n",
        "\n",
        "print('Ends ........... ')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}